{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from wikitext_dataset import Wikitext_2\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "sequence_length = 128\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Wikitext_2(download=True, seq_len=sequence_length, root=\"./\", train=True)\n",
    "valid_data = Wikitext_2(download=True, seq_len=sequence_length, root=\"./\", valid=True)\n",
    "test_data = Wikitext_2(download=True, seq_len=sequence_length, root=\"./\", test=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(valid_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
    "                    weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
    "        else:\n",
    "            return weight.new(self.nlayers, bsz, self.nhid).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(train_data.vocabulary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i, (data, targets) in enumerate(data_loader):\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets.view(-1)).item()\n",
    "    return total_loss / (len(data_loader) * sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(train_data.vocabulary)\n",
    "    for batch, (data, targets) in enumerate(train_loader): \n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            print(\"| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}\".format(\n",
    "                epoch, batch, len(train_loader), lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(train_data.vocabulary)\n",
    "model = RNNModel(\"LSTM\", ntokens, 256, 256, 3, 0.3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = torch.rand(1, 1).mul(ntokens).long()\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = train_data.inverse_vocabulary[s_idx.item()]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " Сズеय川ซこ邱K=しшkহწò्德ณ钱რ空ṅოoتド波t里饾o蝶कܝס0óキ學ǐニάÉ放ǐÁはナК \n",
      "\n",
      "| epoch   1 |   100/ 2844 batches | lr 4.00 | loss  3.88 | ppl    48.25\n",
      "| epoch   1 |   200/ 2844 batches | lr 4.00 | loss  3.38 | ppl    29.26\n",
      "| epoch   1 |   300/ 2844 batches | lr 4.00 | loss  3.30 | ppl    27.03\n",
      "| epoch   1 |   400/ 2844 batches | lr 4.00 | loss  3.27 | ppl    26.30\n",
      "| epoch   1 |   500/ 2844 batches | lr 4.00 | loss  3.27 | ppl    26.30\n",
      "| epoch   1 |   600/ 2844 batches | lr 4.00 | loss  3.22 | ppl    25.04\n",
      "| epoch   1 |   700/ 2844 batches | lr 4.00 | loss  3.22 | ppl    25.14\n",
      "| epoch   1 |   800/ 2844 batches | lr 4.00 | loss  3.20 | ppl    24.50\n",
      "| epoch   1 |   900/ 2844 batches | lr 4.00 | loss  3.19 | ppl    24.28\n",
      "| epoch   1 |  1000/ 2844 batches | lr 4.00 | loss  3.21 | ppl    24.73\n",
      "| epoch   1 |  1100/ 2844 batches | lr 4.00 | loss  3.21 | ppl    24.72\n",
      "| epoch   1 |  1200/ 2844 batches | lr 4.00 | loss  3.20 | ppl    24.56\n",
      "| epoch   1 |  1300/ 2844 batches | lr 4.00 | loss  3.21 | ppl    24.78\n",
      "| epoch   1 |  1400/ 2844 batches | lr 4.00 | loss  3.21 | ppl    24.71\n",
      "| epoch   1 |  1500/ 2844 batches | lr 4.00 | loss  3.20 | ppl    24.48\n",
      "| epoch   1 |  1600/ 2844 batches | lr 4.00 | loss  3.18 | ppl    24.04\n",
      "| epoch   1 |  1700/ 2844 batches | lr 4.00 | loss  3.15 | ppl    23.41\n",
      "| epoch   1 |  1800/ 2844 batches | lr 4.00 | loss  3.03 | ppl    20.62\n",
      "| epoch   1 |  1900/ 2844 batches | lr 4.00 | loss  2.98 | ppl    19.63\n",
      "| epoch   1 |  2000/ 2844 batches | lr 4.00 | loss  2.90 | ppl    18.24\n",
      "| epoch   1 |  2100/ 2844 batches | lr 4.00 | loss  2.86 | ppl    17.49\n",
      "| epoch   1 |  2200/ 2844 batches | lr 4.00 | loss  2.85 | ppl    17.23\n",
      "| epoch   1 |  2300/ 2844 batches | lr 4.00 | loss  2.83 | ppl    17.00\n",
      "| epoch   1 |  2400/ 2844 batches | lr 4.00 | loss  2.81 | ppl    16.59\n",
      "| epoch   1 |  2500/ 2844 batches | lr 4.00 | loss  2.79 | ppl    16.21\n",
      "| epoch   1 |  2600/ 2844 batches | lr 4.00 | loss  2.80 | ppl    16.39\n",
      "| epoch   1 |  2700/ 2844 batches | lr 4.00 | loss  2.79 | ppl    16.36\n",
      "| epoch   1 |  2800/ 2844 batches | lr 4.00 | loss  2.76 | ppl    15.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.18 | valid ppl     3.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " inad . sult ade C o imuree pt tan f t t ped ca iml \n",
      "\n",
      "| epoch   2 |   100/ 2844 batches | lr 4.00 | loss  2.76 | ppl    15.82\n",
      "| epoch   2 |   200/ 2844 batches | lr 4.00 | loss  2.74 | ppl    15.43\n",
      "| epoch   2 |   300/ 2844 batches | lr 4.00 | loss  2.71 | ppl    15.08\n",
      "| epoch   2 |   400/ 2844 batches | lr 4.00 | loss  2.72 | ppl    15.22\n",
      "| epoch   2 |   500/ 2844 batches | lr 4.00 | loss  2.73 | ppl    15.34\n",
      "| epoch   2 |   600/ 2844 batches | lr 4.00 | loss  2.69 | ppl    14.72\n",
      "| epoch   2 |   700/ 2844 batches | lr 4.00 | loss  2.69 | ppl    14.80\n",
      "| epoch   2 |   800/ 2844 batches | lr 4.00 | loss  2.68 | ppl    14.57\n",
      "| epoch   2 |   900/ 2844 batches | lr 4.00 | loss  2.66 | ppl    14.29\n",
      "| epoch   2 |  1000/ 2844 batches | lr 4.00 | loss  2.68 | ppl    14.59\n",
      "| epoch   2 |  1100/ 2844 batches | lr 4.00 | loss  2.66 | ppl    14.36\n",
      "| epoch   2 |  1200/ 2844 batches | lr 4.00 | loss  2.67 | ppl    14.44\n",
      "| epoch   2 |  1300/ 2844 batches | lr 4.00 | loss  2.66 | ppl    14.31\n",
      "| epoch   2 |  1400/ 2844 batches | lr 4.00 | loss  2.65 | ppl    14.09\n",
      "| epoch   2 |  1500/ 2844 batches | lr 4.00 | loss  2.65 | ppl    14.19\n",
      "| epoch   2 |  1600/ 2844 batches | lr 4.00 | loss  2.63 | ppl    13.93\n",
      "| epoch   2 |  1700/ 2844 batches | lr 4.00 | loss  2.64 | ppl    14.00\n",
      "| epoch   2 |  1800/ 2844 batches | lr 4.00 | loss  2.64 | ppl    14.02\n",
      "| epoch   2 |  1900/ 2844 batches | lr 4.00 | loss  2.65 | ppl    14.10\n",
      "| epoch   2 |  2000/ 2844 batches | lr 4.00 | loss  2.62 | ppl    13.74\n",
      "| epoch   2 |  2100/ 2844 batches | lr 4.00 | loss  2.59 | ppl    13.37\n",
      "| epoch   2 |  2200/ 2844 batches | lr 4.00 | loss  2.60 | ppl    13.43\n",
      "| epoch   2 |  2300/ 2844 batches | lr 4.00 | loss  2.59 | ppl    13.38\n",
      "| epoch   2 |  2400/ 2844 batches | lr 4.00 | loss  2.58 | ppl    13.14\n",
      "| epoch   2 |  2500/ 2844 batches | lr 4.00 | loss  2.57 | ppl    13.05\n",
      "| epoch   2 |  2600/ 2844 batches | lr 4.00 | loss  2.60 | ppl    13.42\n",
      "| epoch   2 |  2700/ 2844 batches | lr 4.00 | loss  2.61 | ppl    13.57\n",
      "| epoch   2 |  2800/ 2844 batches | lr 4.00 | loss  2.58 | ppl    13.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.24 | valid ppl     3.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " a therd . th \n",
      " aniatimis r @-mo , @-@0n. ced . pe  \n",
      "\n",
      "| epoch   3 |   100/ 2844 batches | lr 1.00 | loss  2.59 | ppl    13.28\n",
      "| epoch   3 |   200/ 2844 batches | lr 1.00 | loss  2.57 | ppl    13.02\n",
      "| epoch   3 |   300/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.93\n",
      "| epoch   3 |   400/ 2844 batches | lr 1.00 | loss  2.57 | ppl    13.03\n",
      "| epoch   3 |   500/ 2844 batches | lr 1.00 | loss  2.58 | ppl    13.26\n",
      "| epoch   3 |   600/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.75\n",
      "| epoch   3 |   700/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.88\n",
      "| epoch   3 |   800/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.82\n",
      "| epoch   3 |   900/ 2844 batches | lr 1.00 | loss  2.53 | ppl    12.52\n",
      "| epoch   3 |  1000/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.91\n",
      "| epoch   3 |  1100/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.71\n",
      "| epoch   3 |  1200/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.90\n",
      "| epoch   3 |  1300/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.82\n",
      "| epoch   3 |  1400/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.72\n",
      "| epoch   3 |  1500/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.95\n",
      "| epoch   3 |  1600/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.74\n",
      "| epoch   3 |  1700/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.82\n",
      "| epoch   3 |  1800/ 2844 batches | lr 1.00 | loss  2.56 | ppl    12.97\n",
      "| epoch   3 |  1900/ 2844 batches | lr 1.00 | loss  2.57 | ppl    13.08\n",
      "| epoch   3 |  2000/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.84\n",
      "| epoch   3 |  2100/ 2844 batches | lr 1.00 | loss  2.53 | ppl    12.51\n",
      "| epoch   3 |  2200/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.66\n",
      "| epoch   3 |  2300/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.66\n",
      "| epoch   3 |  2400/ 2844 batches | lr 1.00 | loss  2.52 | ppl    12.47\n",
      "| epoch   3 |  2500/ 2844 batches | lr 1.00 | loss  2.52 | ppl    12.43\n",
      "| epoch   3 |  2600/ 2844 batches | lr 1.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   3 |  2700/ 2844 batches | lr 1.00 | loss  2.57 | ppl    13.04\n",
      "| epoch   3 |  2800/ 2844 batches | lr 1.00 | loss  2.54 | ppl    12.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.27 | valid ppl     3.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " cotofingruneciecrion Crkion Pits came y d aret – c \n",
      "\n",
      "| epoch   4 |   100/ 2844 batches | lr 0.25 | loss  2.56 | ppl    12.95\n",
      "| epoch   4 |   200/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.72\n",
      "| epoch   4 |   300/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.65\n",
      "| epoch   4 |   400/ 2844 batches | lr 0.25 | loss  2.55 | ppl    12.76\n",
      "| epoch   4 |   500/ 2844 batches | lr 0.25 | loss  2.57 | ppl    13.02\n",
      "| epoch   4 |   600/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.50\n",
      "| epoch   4 |   700/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.64\n",
      "| epoch   4 |   800/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.62\n",
      "| epoch   4 |   900/ 2844 batches | lr 0.25 | loss  2.51 | ppl    12.31\n",
      "| epoch   4 |  1000/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.73\n",
      "| epoch   4 |  1100/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.52\n",
      "| epoch   4 |  1200/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.72\n",
      "| epoch   4 |  1300/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.65\n",
      "| epoch   4 |  1400/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.57\n",
      "| epoch   4 |  1500/ 2844 batches | lr 0.25 | loss  2.55 | ppl    12.80\n",
      "| epoch   4 |  1600/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.59\n",
      "| epoch   4 |  1700/ 2844 batches | lr 0.25 | loss  2.54 | ppl    12.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  1800/ 2844 batches | lr 0.25 | loss  2.55 | ppl    12.87\n",
      "| epoch   4 |  1900/ 2844 batches | lr 0.25 | loss  2.56 | ppl    12.97\n",
      "| epoch   4 |  2000/ 2844 batches | lr 0.25 | loss  2.55 | ppl    12.75\n",
      "| epoch   4 |  2100/ 2844 batches | lr 0.25 | loss  2.52 | ppl    12.41\n",
      "| epoch   4 |  2200/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.57\n",
      "| epoch   4 |  2300/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.59\n",
      "| epoch   4 |  2400/ 2844 batches | lr 0.25 | loss  2.52 | ppl    12.40\n",
      "| epoch   4 |  2500/ 2844 batches | lr 0.25 | loss  2.52 | ppl    12.39\n",
      "| epoch   4 |  2600/ 2844 batches | lr 0.25 | loss  2.55 | ppl    12.79\n",
      "| epoch   4 |  2700/ 2844 batches | lr 0.25 | loss  2.57 | ppl    13.01\n",
      "| epoch   4 |  2800/ 2844 batches | lr 0.25 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.28 | valid ppl     3.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  alanin becamompalan ched be t , tid , swis Und tr \n",
      "\n",
      "| epoch   5 |   100/ 2844 batches | lr 0.06 | loss  2.56 | ppl    12.88\n",
      "| epoch   5 |   200/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.65\n",
      "| epoch   5 |   300/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.60\n",
      "| epoch   5 |   400/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.73\n",
      "| epoch   5 |   500/ 2844 batches | lr 0.06 | loss  2.56 | ppl    12.98\n",
      "| epoch   5 |   600/ 2844 batches | lr 0.06 | loss  2.52 | ppl    12.45\n",
      "| epoch   5 |   700/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.60\n",
      "| epoch   5 |   800/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.58\n",
      "| epoch   5 |   900/ 2844 batches | lr 0.06 | loss  2.51 | ppl    12.28\n",
      "| epoch   5 |  1000/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.70\n",
      "| epoch   5 |  1100/ 2844 batches | lr 0.06 | loss  2.52 | ppl    12.49\n",
      "| epoch   5 |  1200/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.69\n",
      "| epoch   5 |  1300/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.63\n",
      "| epoch   5 |  1400/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.55\n",
      "| epoch   5 |  1500/ 2844 batches | lr 0.06 | loss  2.55 | ppl    12.76\n",
      "| epoch   5 |  1600/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.56\n",
      "| epoch   5 |  1700/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.65\n",
      "| epoch   5 |  1800/ 2844 batches | lr 0.06 | loss  2.55 | ppl    12.85\n",
      "| epoch   5 |  1900/ 2844 batches | lr 0.06 | loss  2.56 | ppl    12.94\n",
      "| epoch   5 |  2000/ 2844 batches | lr 0.06 | loss  2.54 | ppl    12.73\n",
      "| epoch   5 |  2100/ 2844 batches | lr 0.06 | loss  2.52 | ppl    12.39\n",
      "| epoch   5 |  2200/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.56\n",
      "| epoch   5 |  2300/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.56\n",
      "| epoch   5 |  2400/ 2844 batches | lr 0.06 | loss  2.52 | ppl    12.39\n",
      "| epoch   5 |  2500/ 2844 batches | lr 0.06 | loss  2.52 | ppl    12.39\n",
      "| epoch   5 |  2600/ 2844 batches | lr 0.06 | loss  2.55 | ppl    12.77\n",
      "| epoch   5 |  2700/ 2844 batches | lr 0.06 | loss  2.57 | ppl    13.01\n",
      "| epoch   5 |  2800/ 2844 batches | lr 0.06 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.28 | valid ppl     3.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " os \" Ste othiorer 196 n intrero Sery tescoromarke  \n",
      "\n",
      "| epoch   6 |   100/ 2844 batches | lr 0.02 | loss  2.55 | ppl    12.86\n",
      "| epoch   6 |   200/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.63\n",
      "| epoch   6 |   300/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.60\n",
      "| epoch   6 |   400/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.71\n",
      "| epoch   6 |   500/ 2844 batches | lr 0.02 | loss  2.56 | ppl    12.98\n",
      "| epoch   6 |   600/ 2844 batches | lr 0.02 | loss  2.52 | ppl    12.43\n",
      "| epoch   6 |   700/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.59\n",
      "| epoch   6 |   800/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.57\n",
      "| epoch   6 |   900/ 2844 batches | lr 0.02 | loss  2.51 | ppl    12.27\n",
      "| epoch   6 |  1000/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.69\n",
      "| epoch   6 |  1100/ 2844 batches | lr 0.02 | loss  2.52 | ppl    12.49\n",
      "| epoch   6 |  1200/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.69\n",
      "| epoch   6 |  1300/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.61\n",
      "| epoch   6 |  1400/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.55\n",
      "| epoch   6 |  1500/ 2844 batches | lr 0.02 | loss  2.55 | ppl    12.76\n",
      "| epoch   6 |  1600/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.55\n",
      "| epoch   6 |  1700/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.65\n",
      "| epoch   6 |  1800/ 2844 batches | lr 0.02 | loss  2.55 | ppl    12.86\n",
      "| epoch   6 |  1900/ 2844 batches | lr 0.02 | loss  2.56 | ppl    12.93\n",
      "| epoch   6 |  2000/ 2844 batches | lr 0.02 | loss  2.54 | ppl    12.73\n",
      "| epoch   6 |  2100/ 2844 batches | lr 0.02 | loss  2.52 | ppl    12.39\n",
      "| epoch   6 |  2200/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.55\n",
      "| epoch   6 |  2300/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.58\n",
      "| epoch   6 |  2400/ 2844 batches | lr 0.02 | loss  2.52 | ppl    12.38\n",
      "| epoch   6 |  2500/ 2844 batches | lr 0.02 | loss  2.52 | ppl    12.39\n",
      "| epoch   6 |  2600/ 2844 batches | lr 0.02 | loss  2.55 | ppl    12.77\n",
      "| epoch   6 |  2700/ 2844 batches | lr 0.02 | loss  2.57 | ppl    13.01\n",
      "| epoch   6 |  2800/ 2844 batches | lr 0.02 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.28 | valid ppl     3.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " edhas isovis Tt whe ford a , ; Man Bedeweculullibi \n",
      "\n",
      "| epoch   7 |   100/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.84\n",
      "| epoch   7 |   200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   7 |   300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.59\n",
      "| epoch   7 |   400/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.71\n",
      "| epoch   7 |   500/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.97\n",
      "| epoch   7 |   600/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.44\n",
      "| epoch   7 |   700/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.58\n",
      "| epoch   7 |   800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch   7 |   900/ 2844 batches | lr 0.00 | loss  2.51 | ppl    12.27\n",
      "| epoch   7 |  1000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.69\n",
      "| epoch   7 |  1100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.47\n",
      "| epoch   7 |  1200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.68\n",
      "| epoch   7 |  1300/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   7 |  1400/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.54\n",
      "| epoch   7 |  1500/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.75\n",
      "| epoch   7 |  1600/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.54\n",
      "| epoch   7 |  1700/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.66\n",
      "| epoch   7 |  1800/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   7 |  1900/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.94\n",
      "| epoch   7 |  2000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.73\n",
      "| epoch   7 |  2100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.40\n",
      "| epoch   7 |  2200/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.56\n",
      "| epoch   7 |  2300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   7 |  2400/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   7 |  2500/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   7 |  2600/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.77\n",
      "| epoch   7 |  2700/ 2844 batches | lr 0.00 | loss  2.57 | ppl    13.01\n",
      "| epoch   7 |  2800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.28 | valid ppl     3.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  tag is \n",
      " ( , ifrirrbly thereradorer imatesutyheat \n",
      "\n",
      "| epoch   8 |   100/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   8 |   200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   8 |   300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   8 |   400/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.71\n",
      "| epoch   8 |   500/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.97\n",
      "| epoch   8 |   600/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.43\n",
      "| epoch   8 |   700/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.58\n",
      "| epoch   8 |   800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch   8 |   900/ 2844 batches | lr 0.00 | loss  2.51 | ppl    12.27\n",
      "| epoch   8 |  1000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.69\n",
      "| epoch   8 |  1100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.46\n",
      "| epoch   8 |  1200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.68\n",
      "| epoch   8 |  1300/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   8 |  1400/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   8 |  1500/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.74\n",
      "| epoch   8 |  1600/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   8 |  1700/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.64\n",
      "| epoch   8 |  1800/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   8 |  1900/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.94\n",
      "| epoch   8 |  2000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.72\n",
      "| epoch   8 |  2100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.39\n",
      "| epoch   8 |  2200/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   8 |  2300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.56\n",
      "| epoch   8 |  2400/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   8 |  2500/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   8 |  2600/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.77\n",
      "| epoch   8 |  2700/ 2844 batches | lr 0.00 | loss  2.56 | ppl    13.00\n",
      "| epoch   8 |  2800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.28 | valid ppl     3.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " e eagie 's tous 'useives we cmb apibe d t ltheves  \n",
      "\n",
      "| epoch   9 |   100/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch   9 |   200/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.62\n",
      "| epoch   9 |   300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.59\n",
      "| epoch   9 |   400/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.71\n",
      "| epoch   9 |   500/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.97\n",
      "| epoch   9 |   600/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.43\n",
      "| epoch   9 |   700/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.58\n",
      "| epoch   9 |   800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch   9 |   900/ 2844 batches | lr 0.00 | loss  2.51 | ppl    12.27\n",
      "| epoch   9 |  1000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.68\n",
      "| epoch   9 |  1100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.47\n",
      "| epoch   9 |  1200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.68\n",
      "| epoch   9 |  1300/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch   9 |  1400/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.54\n",
      "| epoch   9 |  1500/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.74\n",
      "| epoch   9 |  1600/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   9 |  1700/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.66\n",
      "| epoch   9 |  1800/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.86\n",
      "| epoch   9 |  1900/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.94\n",
      "| epoch   9 |  2000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.72\n",
      "| epoch   9 |  2100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.39\n",
      "| epoch   9 |  2200/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch   9 |  2300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.56\n",
      "| epoch   9 |  2400/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   9 |  2500/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch   9 |  2600/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.77\n",
      "| epoch   9 |  2700/ 2844 batches | lr 0.00 | loss  2.57 | ppl    13.00\n",
      "| epoch   9 |  2800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.28 | valid ppl     3.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ins re Rlalante u athr th t voiobil the @ buco ion \n",
      "\n",
      "| epoch  10 |   100/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.86\n",
      "| epoch  10 |   200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.62\n",
      "| epoch  10 |   300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.59\n",
      "| epoch  10 |   400/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.71\n",
      "| epoch  10 |   500/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.97\n",
      "| epoch  10 |   600/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.44\n",
      "| epoch  10 |   700/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch  10 |   800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch  10 |   900/ 2844 batches | lr 0.00 | loss  2.51 | ppl    12.27\n",
      "| epoch  10 |  1000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.67\n",
      "| epoch  10 |  1100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.48\n",
      "| epoch  10 |  1200/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.68\n",
      "| epoch  10 |  1300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.61\n",
      "| epoch  10 |  1400/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.54\n",
      "| epoch  10 |  1500/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.75\n",
      "| epoch  10 |  1600/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.54\n",
      "| epoch  10 |  1700/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.66\n",
      "| epoch  10 |  1800/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.85\n",
      "| epoch  10 |  1900/ 2844 batches | lr 0.00 | loss  2.56 | ppl    12.94\n",
      "| epoch  10 |  2000/ 2844 batches | lr 0.00 | loss  2.54 | ppl    12.72\n",
      "| epoch  10 |  2100/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch  10 |  2200/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.55\n",
      "| epoch  10 |  2300/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.57\n",
      "| epoch  10 |  2400/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.38\n",
      "| epoch  10 |  2500/ 2844 batches | lr 0.00 | loss  2.52 | ppl    12.39\n",
      "| epoch  10 |  2600/ 2844 batches | lr 0.00 | loss  2.55 | ppl    12.77\n",
      "| epoch  10 |  2700/ 2844 batches | lr 0.00 | loss  2.57 | ppl    13.00\n",
      "| epoch  10 |  2800/ 2844 batches | lr 0.00 | loss  2.53 | ppl    12.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.28 | valid ppl     3.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " g ± deay thedeus the pps ' = araf booe w s s Dos E \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"sample:\\n\", generate(50), \"\\n\")\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train()\n",
    "    val_loss = evaluate(val_loader)\n",
    "    print(\"-\" * 89)\n",
    "    print(\"| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}\".format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print(\"-\" * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    with torch.no_grad():\n",
    "        print(\"sample:\\n\", generate(50), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open(\"./generated075.txt\", \"w\") as outf:\n",
    "    outf.write(t075)\n",
    "with open(\"./generated1.txt\", \"w\") as outf:\n",
    "    outf.write(t1)\n",
    "with open(\"./generated15.txt\", \"w\") as outf:\n",
    "    outf.write(t15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
